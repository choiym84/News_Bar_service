import logging
import asyncio
import re
from app.utils.headline_crawler import get_naver_headlines
from app.utils.naver_api import search_news_by_keyword
from app.utils.content_crawler import crawl_articles
from app.db.insertData import store_hot_topics_and_return_list, save_article
from app.utils.AI_Model.AI_main import ai_model2
from app.db.findData import find_article_id_by_url
from app.utils.AWS_img import download_image_to_local,upload_image_to_s3_from_url


# from app.utils.AI_Model.hot_topic import generate_responses
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

#1ì°¨ ëª¨ë¸
def request_first_model(titles):
    
    '''
    1ì°¨ AI ëª¨ë¸ ì„í¬íŠ¸.
    '''
    # keywords = generate_responses(titles)
    keywords =  ["êµ­ë¯¼ì˜í˜"]#,"ì´ì¬ëª…","ì§„ë³´ì£¼ì˜","ëŒ€ì„ íˆ¬í‘œ","7ì¸íšŒ","ê¹€ë¬¸ìˆ˜"]#["ê¹€ë¬¸ìˆ˜Â·êµ­í˜ ì§€ë„ë¶€ ë°œì–¸"]#, "ëŒ€ë²•ì›ì¥ íƒ„í•µ ë…¼ë€", "ì´ì¬ëª… ì‚¬ë²•ë¦¬ìŠ¤í¬", "ê¹€ë¬¸ìˆ˜Â·êµ­í˜ ì§€ë„ë¶€ ë°œì–¸", "ê¹€ì •ì€ ì¥ê°‘ë¬´ë ¥ ê°•ì¡°", "ê°€ì§œë‰´ìŠ¤Â·ë‚´ë€ì„¸ë ¥ ê·œì •"]
    keywords = ['4ë…„ ì—°ì„ì œ','5Â·18 ì •ì‹  ìˆ˜ë¡','êµ­ì • ëŠ¥ë ¥ vs ì§„ì§œ ì¼ê¾¼',"ë‚´ë€ìˆ˜ê´´",'ìœ¤ ëŒ€í†µë ¹ íƒˆë‹¹','ì´ì¬ëª… ì§€ì§€ ì„ ì–¸']
    return keywords #ì¼ë‹¨ ê± ë„˜ê¸°ëŠ”ê±°ì§€.




#2ì°¨ ëª¨ë¸
def request_second_model(filtered_articles):
    
    # ë”ë¯¸ ê²°ê³¼ â†’ ê·¸ëƒ¥ 'ì°¬ì„±' / 'ì¤‘ë¦½' / 'ë°˜ëŒ€' ëœë¤ or ê³ ì •ê°’
    '''
    2ì°¨ AI ëª¨ë¸ ì„í¬íŠ¸.
    '''


    for group in filtered_articles:
        for article in filtered_articles[group]:
            article["stance"] = "ì¤‘ë¦½"
    return filtered_articles


def post_to_app_front(results):
    logger.info(f"ğŸ“¡ [ë”ë¯¸] ì•± í”„ë¡ íŠ¸ë¡œ ì „ì†¡ë¨! ê²°ê³¼ ìˆ˜: {sum(len(v) for v in results.values())}")
    print(results)  # ì‹¤ì œë¡œëŠ” requests.post ë“± ì“¸ ìë¦¬


def merge_articles(articles_link, articles):
    # í¬ë¡¤ë§ëœ ê²°ê³¼ë¥¼ link ê¸°ì¤€ìœ¼ë¡œ ë”•ì…”ë„ˆë¦¬í™”
    crawled_dict = {item["link"]: item for item in articles}

    merged = []
    for link_info in articles_link:
        link = link_info["link"]
        crawled_data = crawled_dict.get(link)

        if crawled_data:
            merged_item = {**link_info, **crawled_data}
            merged.append(merged_item)
        # else: í¬ë¡¤ë§ ì‹¤íŒ¨í•œ ê²½ìš°ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ!
    return merged


#ë¶ˆìš©ì–´ txt í˜¸ì¶œ.
def load_stopwords(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        stopwords = [line.strip() for line in f if line.strip()]
    return stopwords


def clean_text(text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ìˆ˜ë¬¸ì, ê¸°í˜¸ ì œê±°
    (í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°±ë§Œ ë‚¨ê¸°ê¸°)
    """
    # í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°± ì™¸ ëª¨ë‘ ì œê±°
    return re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', text)

#í‚¤ì›Œë“œì— ë¶ˆìš©ì–´ ì²˜ë¦¬.
def preprocess_keywords(keywords, stopwords):
    """
    ëª¨ë¸ 1 ê²°ê³¼ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ â†’ ë¶ˆìš©ì–´ ì œê±°ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    processed = []
    for keyword in keywords:
        cleaned_keyword = clean_text(keyword)  
        tokens = cleaned_keyword.strip().split()  # ë„ì–´ì“°ê¸°ë¡œ ë‚˜ëˆ 
        filtered_tokens = [token for token in tokens if token not in stopwords]
        processed_keyword = " ".join(filtered_tokens)  # ë‹¤ì‹œ í•©ì¹˜ê¸°
        if processed_keyword:  # ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì¶”ê°€
            processed.append(processed_keyword)
    return processed


#ì–¸ë¡ ì‚¬ì— ì˜í•œ ì •ì¹˜ì„±í–¥ í•„í„°ë§.
def label_media_bias(data):
    conservative = {"ì¡°ì„ ì¼ë³´", "ì¤‘ì•™ì¼ë³´", "ë™ì•„ì¼ë³´", "ë¬¸í™”ì¼ë³´", "êµ­ë¯¼ì¼ë³´", "tvì¡°ì„ ", "ì±„ë„A"}
    progressive = {"í•œê²¨ë ˆ", "ê²½í–¥ì‹ ë¬¸", "ë¯¸ë””ì–´ì˜¤ëŠ˜", "ì˜¤ë§ˆì´ë‰´ìŠ¤", "MBC", "JTBC", "í”„ë ˆì‹œì•ˆ", "ë¯¼ì¤‘ì˜ì†Œë¦¬"}
    centrist = {"ì„œìš¸ì‹ ë¬¸", "í•œêµ­ì¼ë³´", "ë‚´ì¼ì‹ ë¬¸", "ë‰´ì‹œìŠ¤", "ì—°í•©ë‰´ìŠ¤", "íŒŒì´ë‚¸ì…œë‰´ìŠ¤"}


    new_data = []
    delete_article = 0
    con,cen,pro = 0,0,0

    for article in data:

        if article['publisher'] in conservative:
            article['stance'] = 'ë³´ìˆ˜'
            new_data.append(article)
            con += 1
            
        elif article['publisher'] in progressive:
            article['stance'] = 'ì§„ë³´'
            new_data.append(article)
            pro += 1
            
        elif article['publisher'] in centrist:
            article['stance'] = 'ì¤‘ë¦½'
            new_data.append(article)
            cen += 1
            
        else:
            delete_article += 1

    return new_data,delete_article,con,cen,pro



def start_pipeline():
    logger.info("[Hot Topic Pipeline] ìˆ˜ì§‘ ì‹œì‘ âœ…")
    asdf = 0
    # 1. í—¤ë“œë¼ì¸ í¬ë¡¤ë§
    headlines = get_naver_headlines()
    s = []
    for k in headlines:
        s.append(k["title"])
    logger.info(f"1ë²ˆ ë°ì´í„° í™•ì¸ : {s}")
    print("#############################################")
    # 2. ëª¨ë¸ 1 â†’ í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = request_first_model(s)
    logger.info(f"2ë²ˆ ë°ì´í„° í™•ì¸ : {keywords}")
    keywords = preprocess_keywords(keywords,load_stopwords("app//Stopwords.txt"))
    keywords = store_hot_topics_and_return_list(keywords)

    
    print("#############################################")
    # 3. í‚¤ì›Œë“œ ê¸°ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰
    for keyword in keywords:
        articles_link = search_news_by_keyword(keyword)

        logger.info(f"{keyword}ì˜ ê¸°ì‚¬ ê°¯ìˆ˜ : {len(articles_link)}")
        #ë§í¬ë“¤ë§Œ ë½‘ì•„ë†“ìŒ
        links = [item["link"] for item in articles_link]

        # 4. ë‰´ìŠ¤ ê²€ìƒ‰ + ë³¸ë¬¸ í¬ë¡¤ë§ (ë¹„ë™ê¸°)
        articles = asyncio.run(crawl_articles(links))
        data = merge_articles(articles,articles_link) # ë°ì´í„° í˜•íƒœ{}
        for i in data:
            i['keyword'] = i['keyword']['id']

        # 5. ì–¸ë¡ ì‚¬ í•„í„°ë§

        '''
        ì´ë¶€ë¶„ì€ ì¢€ë” ê³ ë¯¼ì´ í•„ìš”í•¨. í˜„ì¬ëŠ” ì„±í–¥ë³„ë¡œ ê°ê° 3ê°œ, ì´ 9ê°œì˜ ê¸°ì‚¬ë§Œ ë½‘íˆê²Œ ë¨.
        ê·¸ë˜ì„œ í˜¹ì‹œ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì§„ ì•Šì„ê¹Œ? ë¼ëŠ” ê±±ì •ì´ ë“¦. ì´ ë¶€ë¶„ì€ ì¢€ ì²´í¬ê°€ í•„ìš”

        -> ë¨¼ì € ì„±í–¥ì¼ì¹˜ ë¶ˆë¬¸í•˜ê³  í•„í„°ë§ì„ í†µê³¼í•œ ê¸°ì‚¬ëŠ” ëª¨ë‘ dbì— ì €ì¥í•œë‹¤.
        '''
        data,idx,con,cen,pro = label_media_bias(data)

        logger.info(f"ì–¸ë¡ ì‚¬ ë¼ë²¨ë§ì—ì„œ ì´ {idx}ê°œì˜ ê¸°ì‚¬ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")
        logger.info(f"ì´ë²ˆ í‚¤ì›Œë“œì˜ ì„±í–¥ë³„ ê¸°ì‚¬ ê°¯ìˆ˜ : ë³´ìˆ˜ {con}ê°œ, ì¤‘ë„ {cen}ê°œ, ì§„ë³´ {pro}ê°œ") #
        

        new_data=[]
        # 6. ê¸°ì‚¬ ì €ì¥
        for i in data:
            a = find_article_id_by_url(i['link'])
            if a: #ìˆìœ¼ë©´
                pass
            else: #ê¸°ì¡´ì— ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´
                a = save_article(i)

            new_data.append({'article_id':a,'keyword_id':i['keyword'],'stance':i['stance']})

        logger.info("ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ")


        print(len(new_data))
        # 7. ê¸°ì‚¬ ìš”ì•½
        
        data = ai_model2(new_data) #ì„±í–¥ì— ë”°ë¼ 3ê°œ 3ê°œ 3ê°œì˜ ê¸°ì‚¬ë§Œ ë„˜ì–´ì˜¬ê±°ì„.
        # print(len(data)) #ì¼ì¹˜ í•˜ëŠ” ê²ƒë§Œ ë„˜ì–´ì˜´. ì•„ë˜ ì£¼ì„ì´ ë°ì´í„° í˜•íƒœ.

        
        '''
        {'title', 'content', 'publisher', 'reporter', 'link', 'keyword': {'keyword': 'ê¹€ë¬¸ìˆ˜êµ­í˜ ì§€ë„ë¶€ ë°œì–¸', 'id': 105}, 'pub_date', 'stance'}
        '''

        # ê¸°ì‚¬ ì €ì¥í•˜ê³  id ë°›ì•„ì˜´.
        # data = store_filtered_articles_and_return_info(data, keyword['id'])
        # analyzed_results = request_second_model(filtered_articles)

    
    print("[Hot Topic Pipeline] ìˆ˜ì§‘ ì™„ë£Œ âœ…")
    logger.info("[Hot Topic Pipeline] ìˆ˜ì§‘ ì™„ë£Œ âœ…")
