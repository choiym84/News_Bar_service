import os
from app.db.database import SessionLocal
import difflib
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from dotenv import load_dotenv
from app.db.models import ArticleSummary,Article,HotTopic,Bridge,AnalysisSummary

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

DB_CONFIG = {
    'host' : "localhost",
    'user' : "root",
    'password' : "Msj46028759!",
    'database' : "naverapi"
}

SIMILARITY_THRESHOLD = 0.6
#press, title, linkê°€ ìˆëŠ”ëŠ” dbì—ì„œ [input : press 1ì°¨ í•„í„°ë§, text_lines(ocrê²°ê³¼ í•œì¤„ì”©)ë¡œ 2ì°¨ í•„í„°ë§(ìœ ì‚¬ë„ë¡œ) output : link, title]
def get_article_by_press_and_lines_news_details(press, text_lines):
    """
    1) ì£¼ì–´ì§„ ì–¸ë¡ ì‚¬(press)ì— í•´ë‹¹í•˜ëŠ” DB ê¸°ì‚¬ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    2) text_lines(í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸)ì™€ ê° ì œëª©ì„ ë¹„êµí•˜ì—¬ ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ê¸°ì‚¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ë°˜í™˜: (link, date, matched_title) ë˜ëŠ” None
    """
    try:
        db = SessionLocal()

        # 1ì°¨: ì–¸ë¡ ì‚¬ í•„í„°ë§

        results = db.query(Article).filter(Article.publisher == press).all()

        # query = "SELECT title, link FROM news_details WHERE press = %s"
        # cursor.execute(query, (press,))
        # results = cursor.fetchall()
        # cursor.close()
        # conn.close()

        if not results:
            print(f"âŒ DBì— '{press}' ì–¸ë¡ ì‚¬ì˜ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # 2ì°¨: text_linesì™€ ì œëª© ë¹„êµ
        best_match = None
        highest_sim = 0.0
        for row in results:
            candidate_title = row.title
            for line in text_lines:
                sim = difflib.SequenceMatcher(None, line, candidate_title).ratio()
                if sim > highest_sim:
                    highest_sim = sim
                    best_match = row

        

        print(f"âœ… ìµœê³  ìœ ì‚¬ë„: {highest_sim:.2f}")
        if best_match and highest_sim >= SIMILARITY_THRESHOLD:
            return best_match.url, best_match.title, best_match.publish_date
        else:
            print("âš ï¸ ìœ ì‚¬í•œ ì œëª©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

    except Exception as e:
        print("MySQL ì¡°íšŒ ì‹¤íŒ¨:", e)
        return None
    
    finally:
        db.close()

#title, link, dateê°€ ìˆëŠ”ëŠ” dbì—ì„œ í™•ì •ëœ ì œëª©ìœ¼ë¡œ 1ì°¨ í•„í„°ë§ output : link, date
# def get_article_url_by_title_fuzzy(ocr_title):
#     try:
#         conn = mysql.connector.connect(
#             host="localhost",
#             user="root",
#             password="Msj46028759!",
#             database="naverapi"
#         )
#         cursor = conn.cursor(dictionary=True)

#         # ğŸ”„ LIKE ì—†ì´ ì „ì²´ ì œëª© ë‹¤ ê°€ì ¸ì˜¤ê¸°
#         query = "SELECT title, link, date FROM articles"
#         cursor.execute(query)
#         results = cursor.fetchall()

#         cursor.close()
#         conn.close()

#         if not results:
#             print("âŒ DBì— ì œëª©ì´ í•˜ë‚˜ë„ ì—†ì–´ìš”.")
#             return None

#         # ğŸ” OCR ì œëª©ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì œëª© ì°¾ê¸°
#         best_match = None
#         highest_sim = 0.0
#         for row in results:
#             candidate_title = row["title"]
#             sim = difflib.SequenceMatcher(None, ocr_title, candidate_title).ratio()
#             if sim > highest_sim:
#                 highest_sim = sim
#                 best_match = row

#         print(f"âœ… ìµœê³  ìœ ì‚¬ë„: {highest_sim:.2f}")
#         if best_match and highest_sim > 0.6:  # ìœ ì‚¬ë„ ê¸°ì¤€ ì„ê³„ê°’
#             return best_match["link"], best_match["date"]
#         else:
#             print("âš ï¸ ìœ ì‚¬í•œ ì œëª©ì´ ì—†ì–´ìš”.")
#             return None

#     except Exception as e:
#         print("MySQL ì¡°íšŒ ì‹¤íŒ¨:", e)
#         return None


def crawl_naver_news_article(url):
    """
    ì£¼ì–´ì§„ ë„¤ì´ë²„ ë‰´ìŠ¤ URLì—ì„œ ì œëª©, ì–¸ë¡ ì‚¬, ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë°˜í™˜: dict {title, press, content} ë˜ëŠ” None
    """
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        # ì œëª© ì¶”ì¶œ
        title_tag = soup.find("meta", property="og:title")
        title = title_tag["content"].strip() if title_tag else "ì œëª© ì—†ìŒ"

        # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
        press_tag = soup.find("meta", {"name": "twitter:creator"})
        press = press_tag["content"].strip() if press_tag else "ì–¸ë¡ ì‚¬ ì •ë³´ ì—†ìŒ"

        # ë³¸ë¬¸ ì¶”ì¶œ
        content_div = soup.select_one("article#dic_area")
        content = content_div.get_text(separator="\n", strip=True) if content_div else "ë³¸ë¬¸ ì—†ìŒ"

        return {"title": title, "press": press, "content": content}

    except Exception as e:
        print(f"âŒ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {url} / {e}")
        return None

def crawl_naver_news_article_url(url):
    """
    ì£¼ì–´ì§„ ë„¤ì´ë²„ ë‰´ìŠ¤ URLì—ì„œ
      â€¢ ì œëª©(title)
      â€¢ ì–¸ë¡ ì‚¬(press)
      â€¢ ë³¸ë¬¸(content)
      â€¢ ì‘ì„±ì¼ì‹œ(date: datetime)
    ë¥¼ ì¶”ì¶œí•´ dict ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì‹¤íŒ¨ ì‹œ None ë°˜í™˜.
    """
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        # ì œëª©
        title_tag = soup.find("meta", property="og:title")
        title = title_tag.get("content", "").strip() if title_tag else "ì œëª© ì—†ìŒ"

        # ì–¸ë¡ ì‚¬
        press_tag = soup.find("meta", {"name": "twitter:creator"})
        press = press_tag.get("content", "").strip() if press_tag else "ì–¸ë¡ ì‚¬ ì •ë³´ ì—†ìŒ"

        # ë³¸ë¬¸
        content_div = soup.select_one("article#dic_area")
        content = content_div.get_text(separator="\n", strip=True) if content_div else "ë³¸ë¬¸ ì—†ìŒ"

        # ì‘ì„±ì¼ì‹œ íŒŒì‹± (span._ARTICLE_DATE_TIME ì˜ data-date-time ì†ì„±)
        date = None
        dt_span = soup.select_one("span._ARTICLE_DATE_TIME")
        if dt_span and dt_span.has_attr("data-date-time"):
            dt_str = dt_span["data-date-time"].strip()  # ì˜ˆ: "2025-05-07 11:28:15"
            try:
                date = datetime.strptime(dt_str, "%Y-%m-%d %H:%M:%S")
            except ValueError:
                date = None

        return {
            "title":   title,
            "press":   press,
            "content": content,
            "date":    date
        }

    except Exception as e:
        print(f"âŒ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {url} / {e}")
        return None
    

# ì˜ˆì‹œ í˜¸ì¶œ
if __name__ == '__main__':
    # OCR/Extraction ë‹¨ê³„ì—ì„œ ì–»ì€ pressì™€ text_lines
    press = 'í•œêµ­ê²½ì œ'
    text_lines = [
        "N ë‰´ìŠ¤ ì—”í„° ìŠ¤í¬ì¸  í”„ë¦¬ë¯¸ì—„",
        "ì •ì¹˜ ê²½ì œ ì‚¬íšŒ ìƒí™œ/ë¬¸í™” IT/ê³¼í•™ ì„¸ê³„ ë­í‚¹",
        "í•œêµ­ê²½ì œ + êµ¬ë…",
        "í…ŒìŠ¬ë¼, êµ¬ê¸€ì„œ DOGE ê²€ìƒ‰ ë§ì„ìˆ˜ë¡ ì£¼ê°€ â€˜íƒ€ê²©â€™",
        "ì…ë ¥ 2025.03.10. ì˜¤í›„ 9:55",
        "ìˆ˜ì • 2025.03.10. ì˜¤í›„ 10:09",
        "ê¸°ì‚¬ì›ë¬¸",
        "ê¹€ì •ì•„ ê¸°ì",
        "2 ëŒ“ê¸€",
        "DOGEê°€ í…ŒìŠ¬ë¼ì˜ ìƒˆë¡œìš´ ìœ„í—˜ ìš”ì†Œë¡œ ë“±ì¥",
        "êµ¬ê¸€ ê²€ìƒ‰ì— ë¨¸ìŠ¤í¬ ì •ì¹˜ í™œë™ ë¦¬ìŠ¤í¬ ë°˜ì˜",
        "ì´ ê¸°ì‚¬ëŠ” êµ­ë‚´ ìµœëŒ€ í•´ì™¸ íˆ¬ìì •ë³´ í”Œë«í¼ í•œê²½ ê¸€ë¡œë²Œë§ˆì¼“ì— ê²Œì¬ëœ ê¸°ì‚¬ì…ë‹ˆë‹¤.",
        "JSK R",
        "UP TESLA",
        "TS NAKE NO CONFLICT SELL TESLA Your",
        "IS AGAIN SANE OF Not Your",
        "INTEREST (1)",
        "FUCK",
        "ELON",
        "DEPORT FUER moog",
        "â€œECONOMIC",
        "COCCAPAE",
        "NECESSARY"
    ]

    result = get_article_by_press_and_lines_news_details(press, text_lines)
    print(result)
    if result:
        url, date = result
        print("ğŸ“° ë§¤ì¹­ëœ URL:", url)
        print("ğŸ•’ ì‘ì„±ì‹œê°:", date)

        # í¬ë¡¤ë§ìœ¼ë¡œ ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ
        article_info = crawl_naver_news_article(url)
        if article_info:
            print("\n=== ê¸°ì‚¬ ì •ë³´ ===")
            print("ğŸ“Œ ì œëª©:", article_info["title"])
            print("ğŸ¢ ì–¸ë¡ ì‚¬:", article_info["press"])
            print("ğŸ“ ë³¸ë¬¸ ë‚´ìš©:\n", article_info["content"])
        else:
            print("âŒ ê¸°ì‚¬ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆì–´ìš”.")
    else:
        print("âŒ ìœ ì‚¬í•œ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")






